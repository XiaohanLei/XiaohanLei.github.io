\section{Introduction}

The quest for robotic systems capable of human-level dexterity represents a grand challenge in Embodied Artificial Intelligence. Dexterous robotic hands, with their high degrees of freedom (DoF), hold the potential to perform a vast array of complex, contact-rich manipulation tasks currently beyond the reach of simpler parallel-jaw grippers. Imitation learning, particularly in its offline formulation, has emerged as a promising paradigm for teaching such skills by leveraging large-scale datasets of human demonstrations~\cite{o2024open, team2024octo, khazatsky2024droid, dasari2019robonet, walke2023bridgedata, mandlekar2018roboturk, fang2023rh20t, jiang2025kaiwu, bu2025agibot}. A critical bottleneck, however, lies in the cross-embodiment transfer of these skills: how can a robot effectively learn from heterogeneous demonstrations, given the significant differences in morphology, kinematics, and sensory feedback~\cite{wang2024scaling, zhou2025mitigating, chen2025vidbot, yang2025egovla}? While recent Vision-Language-Action (VLA) models have made strides in general-purpose robotics~\cite{zitkovich2023rt, zheng2024tracevla, kim2024openvla, wen2025tinyvla, zhao2025cot}, they predominantly rely on 2D visual inputs, which often fail to capture the intricate 3D spatial relationships essential for precise dexterous manipulation~\cite{qian20253d, li2025bridgevla, li2025pointvla, huang2023voxposer, zhen20243d}. This work tackles the challenge of cross-embodiment imitation for dexterous hands by directly learning from 3D point cloud observations, proposing a fundamental shift in how we represent and process robotic actions.

A dominant paradigm in recent policy learning is action chunking, where a model predicts a sequence of future actions $(T, D_a)$~\cite{chi2023diffusion, zhao2023learning}. As in Figure~\ref{fig:teaser} (a), this \textbf{temporal-centric perspective}, which treats each $D_a$-dimensional vector as a token in a temporal sequence, is effective for low-dimensional systems. However, this representation faces a fundamental challenge when scaling to high-DoF, heterogeneous manipulators. As the action dimension $D_a$ grows (\textit{e.g.}, from a 7-DoF robot arm to a 24-DoF dexterous hand), the model must learn complex, implicit correlations within a monolithic feature vector. More critically, this fixed-dimensional view provides no natural mechanism for cross-embodiment transfer, as different morphologies cannot be directly compared. 

This paper challenges this conventional wisdom by reframing the action representation problem. We propose a \textbf{structural-centric perspective}, modeling actions as a sequence of $D_a$ embodied joints, where each joint's feature is its trajectory over the time horizon $T$, \textit{i.e.}, $(D_a, T)$, as shown in Figure~\ref{fig:teaser} (b). This structural view directly addresses the heterogeneity problem: the sequence length $D_a$ can now vary between embodiments, a property that Transformer architectures handle natively~\cite{vaswani2017attention}. This allows a policy to learn transferable skills by finding functional similarities between corresponding joints. At the same time, this formulation also allows the model to learn a compressed representation of motion primitives for each joint, as time is now treated as the feature dimension.


We introduce the Structural Action Transformer (SAT), a novel 3D dexterous manipulation policy built upon this structural-centric representation.
Our model takes raw 3D point cloud observations and natural language instructions as input.
The 3D scene is processed by a hierarchical tokenizer that uses Farthest Point Sampling (FPS) and PointNets~\cite{qi2017pointnet} to extract both local geometric tokens and a single global scene token.
These are combined with language features from a T5 encoder~\cite{raffel2020exploring} to form a multi-modal observation sequence.
This sequence conditions a Diffusion Transformer (DiT)~\cite{peebles2023scalable} that operates directly on our $(D_a, T)$ structural action representation.
We treat the $D_a$ joints as a variable-length sequence, where each token represents a compressed temporal trajectory for a single joint.
To explicitly encode structural priors and manage heterogeneity, we introduce an Embodied Joint Codebook.
This codebook provides learnable embeddings for each joint based on its morphological properties, enabling the model to identify functional correspondences across different embodiments.
The policy is trained to generate the entire action chunk by learning a conditional velocity field via a continuous-time flow matching objective~\cite{lipman2022flow}, with the final action produced by an ODE solver.

We demonstrate the efficacy of this approach by pre-training on large-scale, heterogeneous datasets of both human and robot demonstrations~\cite{liu2022hoi4d, grauman2024ego, pan2023aria, fourier2025actionnet, wang2024dexcap} and then fine-tuning it on a suite of challenging simulation benchmarks~\cite{rajeswaran2017learning, bao2023dexart, chen2023bi} and real-world bimanual manipulation tasks.
Our extensive experiments show that this structural-centric paradigm consistently outperforms strong baselines~\cite{chi2023diffusion, wang2024scaling, zheng2025universal, ze20243d, yan2025maniflow} and achieves superior sample efficiency, proving its effectiveness in cross-embodiment skill transfer.
Fundamentally, our work is the first to successfully implement a policy that tokenizes actions along the structural dimension, offering a new and scalable path toward learning generalist policies for a diverse ecosystem of high-DoF, heterogeneous manipulators.

